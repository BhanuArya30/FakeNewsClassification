{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e2876d",
   "metadata": {},
   "source": [
    "# Identify Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1997c67",
   "metadata": {},
   "source": [
    "High level approach:\n",
    "1. Install prequisites\n",
    "2. Load data\n",
    "3. Prepare data\n",
    "4. Model Training\n",
    "5. Evaluate model\n",
    "6. Infer test data\n",
    "7. Save results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c051e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install packages that are not installed by default, uncomment the last two lines of this cell \n",
    "# and replace <package list> with a list of needed packages.\n",
    "# This will ensure the notebook has all the dependencies and works everywhere\n",
    "\n",
    "#!mamba install <package list>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee26cf",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b8767c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mamba install --no-update transformers -y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d8f706",
   "metadata": {},
   "source": [
    "***Notes:*** \n",
    "Installing huggingface transfomers library for loading transformer based model for sequence classification of fake news data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648dc72e",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b615c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports for loading data\n",
    "import os\n",
    "from shutil import unpack_archive\n",
    "import pandas as pd\n",
    "\n",
    "# Imports for helper functions\n",
    "import time\n",
    "from typing import Optional, Callable, Any\n",
    "\n",
    "# Imports for data preparation and preprocessing\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Imports for evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "# Imports for model fine-tuning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option(\"display.max_columns\", 101)\n",
    "pd.set_option(\"display.max_colwidth\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9302a7b",
   "metadata": {},
   "source": [
    "***Notes:***\n",
    "\n",
    "Generally, would create a requirements.txt file which contains required libraries for install. This  ensures consistency library versioning across the team and also comes handy in while deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109deedf",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad697a9",
   "metadata": {},
   "source": [
    "***Notes:***\n",
    "\n",
    "These helpfer functions would generally be moved to a common utility file to be shared across team members, this saves time in common functions used across the components. It also ensures consistency and eliminates possiblity of redudant code. Here adding a seperate section for it for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c45a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> str:\n",
    "    \"\"\"\n",
    "    Get the appropriate device (CPU, CUDA GPU, or MPS) for running PyTorch operations.\n",
    "\n",
    "    Returns:\n",
    "        str: The selected device ('cuda', 'mps', or 'cpu').\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_built():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31fe561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set a global random seed for NumPy and PyTorch.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to set.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Set seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Set seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072189d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_it(func: Callable[..., Any]) -> Callable[..., Any]:\n",
    "    \"\"\"\n",
    "    A decorator that measures the time taken to execute a function and prints the elapsed time.\n",
    "\n",
    "    Args:\n",
    "        func (Callable): The function to be decorated.\n",
    "\n",
    "    Returns:\n",
    "        Callable: The decorated function.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Wrapper function to measure the execution time of the decorated function.\n",
    "\n",
    "        Args:\n",
    "            *args: Positional arguments passed to the decorated function.\n",
    "            **kwargs: Keyword arguments passed to the decorated function.\n",
    "\n",
    "        Returns:\n",
    "            Any: The result returned by the decorated function.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"{func.__name__} took {elapsed_time:.4f} seconds to run.\")\n",
    "        return result\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff1992c",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda59a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this cell block to download and extract dataset\n",
    "# !wget 'https://hr-projects-assets-prod.s3.amazonaws.com/4j8je858bmi/ed0142953e1c6f8c928ef63bcb269cdc/train.zip'\n",
    "# !wget 'https://hr-projects-assets-prod.s3.amazonaws.com/4j8je858bmi/dc5feebc74511aeb9753b5074b98a8fe/test.zip'\n",
    "\n",
    "# print('Extracting Train Dataset :')\n",
    "# unpack_archive('train.zip', '')\n",
    "\n",
    "# print('Extracting Test Dataset :')\n",
    "# unpack_archive('test.zip', '')\n",
    "\n",
    "# # Remove zip files\n",
    "# os.remove('train.zip')\n",
    "# os.remove('test.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f43ec0",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca6f54",
   "metadata": {},
   "source": [
    "Column | Description\n",
    ":---|:---\n",
    "`id` | Unique ID of the News Article\n",
    "`title` | Title of the News Article\n",
    "`content` | Body of the News Article\n",
    "`tags` | A string of comma-separated tags associated with the news article.\n",
    "`category` | Category of the news article. (`1` - `fake`, `0` - `reliable`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb5c2f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>God's 10 Commandments Under Attack in Montana</td>\n",
       "      <td>(Photo: Screengrab/NBC Montana) Philip Klevmoen of Gods10.com shows local media the vandalism do...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Steven Curtis Chapman New Album 'The Glorious Unfolding' to be Released October 1</td>\n",
       "      <td>Christian contemporary artist Steven Curtis Chapman announced his new album The Glorious Unfoldi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Casey Anthony Trial: Closing Arguments Begin; Molestation Claim Thrown Out</td>\n",
       "      <td>There is no evidence that Casey Anthony, who is accused of murdering her daughter Caylee Anthony...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspartame-Induced Fibromyalgia</td>\n",
       "      <td>Below is an approximation of this video’s audio content. To see any graphs, charts, graphics, im...</td>\n",
       "      <td>fibromyalgia, aspartame, sweeteners, Volume 11, artificial sweeteners, sugar, pain, NutraSweet, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Religious Leaders 'March' on Federal Reserve in 'Occupy the Dream' Movement</td>\n",
       "      <td>The “Occupy” movement is back, and this time it joins forces with progressive religious leaders ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   0   \n",
       "1   1   \n",
       "2   2   \n",
       "3   3   \n",
       "4   4   \n",
       "\n",
       "                                                                               title  \\\n",
       "0                                      God's 10 Commandments Under Attack in Montana   \n",
       "1  Steven Curtis Chapman New Album 'The Glorious Unfolding' to be Released October 1   \n",
       "2         Casey Anthony Trial: Closing Arguments Begin; Molestation Claim Thrown Out   \n",
       "3                                                     Aspartame-Induced Fibromyalgia   \n",
       "4        Religious Leaders 'March' on Federal Reserve in 'Occupy the Dream' Movement   \n",
       "\n",
       "                                                                                               content  \\\n",
       "0  (Photo: Screengrab/NBC Montana) Philip Klevmoen of Gods10.com shows local media the vandalism do...   \n",
       "1  Christian contemporary artist Steven Curtis Chapman announced his new album The Glorious Unfoldi...   \n",
       "2  There is no evidence that Casey Anthony, who is accused of murdering her daughter Caylee Anthony...   \n",
       "3  Below is an approximation of this video’s audio content. To see any graphs, charts, graphics, im...   \n",
       "4  The “Occupy” movement is back, and this time it joins forces with progressive religious leaders ...   \n",
       "\n",
       "                                                                                                  tags  \\\n",
       "0                                                                                                  NaN   \n",
       "1                                                                                                  NaN   \n",
       "2                                                                                                  NaN   \n",
       "3  fibromyalgia, aspartame, sweeteners, Volume 11, artificial sweeteners, sugar, pain, NutraSweet, ...   \n",
       "4                                                                                                  NaN   \n",
       "\n",
       "   category  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The training dataset containing the news articles and corresponding\n",
    "# categories is already loaded below.\n",
    "train_0 = pd.read_csv(os.path.join(\"train\",\"train_0.csv\"))\n",
    "train_1 = pd.read_csv(os.path.join(\"train\",\"train_1.csv\"))\n",
    "df = df = pd.concat([train_0, train_1], axis=0, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce18cec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50165, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7d75079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving training DataFrame for future use\n",
    "df.to_csv(os.path.join(\"train\",\"data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8065d689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "def read_csv(file_path: str, encoding: str = 'utf-8') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a CSV file from the specified path with the option to specify the encoding.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file to be read.\n",
    "        encoding (str, optional): The encoding to use for reading the CSV file. Default is 'utf-8'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or None: A pandas DataFrame containing the data from the CSV file if\n",
    "        successfully read. Returns None if an error occurs during reading.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try reading the CSV file using the provided file path and encoding\n",
    "        df = pd.read_csv(file_path, encoding=encoding)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the CSV file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "data_path = r'train/data.csv'\n",
    "df = read_csv(data_path)\n",
    "\n",
    "if df is not None:\n",
    "    print(\"CSV file loaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51a9eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - comment before full training\n",
    "# Using a sample of the data for training to speed up the process to overcome compute limitations (running on CPU). \n",
    "\n",
    "# Randomly sample 25% data from the DataFrame for training\n",
    "# sample_len = int(len(df)*0.25)\n",
    "# df = df.sample(sample_len)\n",
    "\n",
    "df = df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88570ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_dataframe(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Concatenates the 'title' and 'content' columns of a DataFrame row.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A row from a DataFrame containing 'title' and 'content' columns.\n",
    "\n",
    "    Returns:\n",
    "        str: Concatenated string of 'title' and 'content'.\n",
    "    \"\"\"\n",
    "    return row['title'] + ' ' + row['content']\n",
    "\n",
    "df['text'] = df.apply(reformat_dataframe, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce6b4b1",
   "metadata": {},
   "source": [
    "***Notes:*** \n",
    "\n",
    "Concatenating the columns of title and content into a single column to be used for classification purposes. The hypothesis is that fake news is generally propagated by a catchy title followed by fake content. Thus, the title may contain patterns that can be used to identify if it is fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45d0ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverse_label_mappings(label_to_idx: dict[str, int]) -> dict[int, str]:\n",
    "    \"\"\"\n",
    "    Create inverse mappings between label names and label indices.\n",
    "\n",
    "    Args:\n",
    "        label_to_idx (dict[str, int]): A dictionary where keys are label names (strings)\n",
    "            and values are corresponding label indices (integers).\n",
    "\n",
    "    Returns:\n",
    "        dict[int, str]: A dictionary where keys are label indices (integers)\n",
    "        and values are corresponding label names (strings).\n",
    "    \"\"\"\n",
    "    idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    return idx_to_label\n",
    "\n",
    "label_to_idx = { 'real': 0, 'fake': 1 }\n",
    "idx_to_label = create_inverse_label_mappings(label_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62925ebe",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a88928",
   "metadata": {},
   "source": [
    "***Notes:***\n",
    "It useful to understand data provenance - how data was acquired, processed in the data pipeline before using for modelling. This would enable context aware processing and if there were any loss of context or special consideration requried for pre-processing. For e.g., data might be jumbled when passing through OCR.\n",
    "\n",
    "Here, I'm using a text standard pre-processing due to lack context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4cd65ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.3 ms, sys: 1.07 ms, total: 25.4 ms\n",
      "Wall time: 24.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses a text by:\n",
    "    1. Removing newline characters '\\n' from the text.\n",
    "    2. Removing extra whitespaces.\n",
    "    3. Removing square brackets and their contents (e.g., \"[...]\").\n",
    "    4. Applying NFC normalization.\n",
    "    5. Cleaning up special characters and extra spaces.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    # Remove newline characters '\\n' from the text\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove square brackets and their contents (e.g., \"[...]\")\n",
    "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
    "    # Apply NFC normalization\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    # Clean up special characters and extra spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8df8c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_split(X: pd.Series, y: pd.Series, test_size: float = 0.2) -> tuple:\n",
    "    \"\"\"\n",
    "    Split input data into training and validation sets.\n",
    "\n",
    "    Parameters:\n",
    "        X (pd.Series): The feature data.\n",
    "        y (pd.Series): The target data.\n",
    "        test_size (float, optional): The proportion of the data to include in the validation split.\n",
    "            Defaults to 0.2.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing X_train, X_val, y_train, and y_val.\n",
    "    \"\"\"\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "X, y = df['text'], df['category']\n",
    "X_train, X_val, y_train, y_val = train_validation_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a140896e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f6232",
   "metadata": {},
   "source": [
    "***Notes:***\n",
    "\n",
    "Model Selection: For the classification task, the distilled version of BERT (Bidirectional Encoder Representations from Transformers), known as distillBERT, is utilized. Using distillBERT provides a lighter alternative to BERT to overcome computational limitations (CPU) during training.\n",
    "\n",
    "Model Baseline: Generally, before training a deep learning-based model, we would train a TF-IDF-based statistical model, such as Naive Bayes or Support Vector Classifier, to establish a baseline. These models are faster to run and require less computational resources but may face challenges in generalizing in certain scenarios. However, for the sake of expediency, I have directly chosen to evaluate distillBERT.\n",
    "\n",
    "Object-Oriented: Here, I have implemented an object-oriented code structure to demonstrate the approach I would take in production pipelines. Additionally, I would also write unit tests (which were skipped due to time constraints), maintain centralized control of the code flow, implement thorough logging, and utilize modular Python files within an organized project structure instead of a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46364636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier:\n",
    "    \"\"\"\n",
    "    Text classifier using Transformers for sequence classification.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int, optional): Number of classes for classification. Default is 2.\n",
    "        batch_size (int, optional): Batch size for training. Default is 8.\n",
    "        learning_rate (float, optional): Learning rate for optimizer. Default is 2e-5.\n",
    "        model_directory (str, optional): Pretrained model directory. Default is \"distilbert-base-uncased\".\n",
    "        max_length (int, optional): Maximum sequence length. Default is 512.\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer (AutoTokenizer): Tokenizer for text processing.\n",
    "        model (AutoModelForSequenceClassification): Pretrained model for classification.\n",
    "        device (str): Device for model training (e.g., 'cpu' or 'cuda').\n",
    "        optimizer (AdamW): Optimizer for model training.\n",
    "        criterion (nn.CrossEntropyLoss): Loss criterion.\n",
    "\n",
    "    Methods:\n",
    "        preprocess_data: Preprocesses input text data.\n",
    "        train: Trains the model on the provided data.\n",
    "        predict: Predicts class labels for input texts.\n",
    "        save_model: Saves the trained model checkpoint.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 2,\n",
    "        batch_size: int = 8,\n",
    "        learning_rate: float = 2e-5,\n",
    "        model_directory: str = \"distilbert-base-uncased\",\n",
    "        max_length: int = 512,\n",
    "    ):\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_directory, use_fast=True)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_directory, num_labels=num_classes\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def preprocess_data(self, texts):\n",
    "        \"\"\"\n",
    "        Preprocesses the input texts.\n",
    "\n",
    "        Args:\n",
    "            texts (list of str): The input texts to be preprocessed.\n",
    "\n",
    "        Returns:\n",
    "            input_ids (torch.Tensor): The input IDs after tokenization and encoding.\n",
    "            attention_masks (torch.Tensor): The attention masks for input texts.\n",
    "        \"\"\"\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for text in texts:\n",
    "            encoded_dict = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                truncation=\"longest_first\",\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            input_ids.append(encoded_dict[\"input_ids\"])\n",
    "            attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "        return input_ids, attention_masks\n",
    "\n",
    "    def train(\n",
    "        self, train_texts, train_labels, val_texts, val_labels, num_epochs: int = 1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains the model on the provided training data and evaluates it on the validation data.\n",
    "\n",
    "        Args:\n",
    "            train_texts (list of str): The training texts.\n",
    "            train_labels (list of int): The training labels.\n",
    "            val_texts (list of str): The validation texts.\n",
    "            val_labels (list of int): The validation labels.\n",
    "            num_epochs (int, optional): The number of training epochs. Default is 3.\n",
    "        \"\"\"\n",
    "        input_ids, attention_masks = self.preprocess_data(train_texts)\n",
    "        labels = torch.tensor(train_labels)\n",
    "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        val_input_ids, val_attention_masks = self.preprocess_data(val_texts)\n",
    "        val_labels = torch.tensor(val_labels)\n",
    "        val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset, batch_size=self.batch_size, shuffle=False\n",
    "        )\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0.0\n",
    "            batch_count = 0\n",
    "            # Create a tqdm progress bar for the training loop\n",
    "            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                batch_count += 1\n",
    "                batch = tuple(t.to(self.device) for t in batch)\n",
    "                input_ids, attention_mask, label = batch\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(\n",
    "                    input_ids, attention_mask=attention_mask, labels=label\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            average_loss = total_loss / len(dataloader)\n",
    "\n",
    "            # Validation loop\n",
    "            self.model.eval()\n",
    "            total_val_loss = 0.0\n",
    "            val_batch_count = 0\n",
    "            val_progress_bar = tqdm(val_dataloader, desc=f\"Validation\")\n",
    "            with torch.no_grad():\n",
    "                for batch in val_progress_bar:\n",
    "                    val_batch_count += 1\n",
    "                    batch = tuple(t.to(self.device) for t in batch)\n",
    "                    input_ids, attention_mask, label = batch\n",
    "\n",
    "                    outputs = self.model(\n",
    "                        input_ids, attention_mask=attention_mask, labels=label\n",
    "                    )\n",
    "                    val_loss = outputs.loss\n",
    "                    total_val_loss += val_loss.item()\n",
    "\n",
    "            average_val_loss = total_val_loss / len(val_dataloader)\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {average_loss:.4f} - Val Loss: {average_val_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "            self.save_model(epoch, average_loss, average_val_loss)\n",
    "\n",
    "    def predict(self, texts):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for the input texts.\n",
    "\n",
    "        Args:\n",
    "            texts (list of str): The texts for which predictions are to be made.\n",
    "\n",
    "        Returns:\n",
    "            predictions (list of int): The predicted class labels.\n",
    "        \"\"\"\n",
    "        input_ids, attention_masks = self.preprocess_data(texts)\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        predictions = []\n",
    "        progress_bar = tqdm(dataloader, desc=\"Predicting\")\n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                batch = tuple(t.to(self.device) for t in batch)\n",
    "                input_ids, attention_mask = batch\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "                predicted_classes = torch.argmax(probabilities, dim=1).tolist()\n",
    "                predictions.extend(predicted_classes)\n",
    "        return predictions\n",
    "\n",
    "    def save_model(self, epoch, train_loss, val_loss, model_path: str = \"model\"):\n",
    "        \"\"\"\n",
    "        Saves the trained model checkpoint.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): The current training epoch.\n",
    "            train_loss (float): The training loss at the end of the epoch.\n",
    "            val_loss (float): The validation loss at the end of the epoch.\n",
    "            model_path (str, optional): Directory to save the model checkpoint. Default is 'model'.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(model_path):\n",
    "            os.makedirs(model_path)\n",
    "\n",
    "        model_filename = (\n",
    "            f\"epoch_{epoch+1}_trainloss_{train_loss:.2f}_valloss_{val_loss:.2f}\"\n",
    "        )\n",
    "        model_filename = model_filename.replace(\".\", \"-\")\n",
    "\n",
    "        model_filepath = os.path.join(model_path, model_filename)\n",
    "        self.model.save_pretrained(model_filepath)\n",
    "        self.tokenizer.save_pretrained(model_filepath)\n",
    "        print(f\"Saved model checkpoint: {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e542762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = X_train.to_list()\n",
    "val_texts = X_val.to_list()\n",
    "train_labels = y_train.to_list()\n",
    "val_labels = y_val.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "627d1334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  40%|████      | 4/10 [00:31<00:45,  7.66s/it]"
     ]
    }
   ],
   "source": [
    "# Begin Training\n",
    "model = TextClassifier()\n",
    "model.train(train_texts, train_labels, val_texts, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7eb479",
   "metadata": {},
   "source": [
    "***Notes:***\n",
    "\n",
    "Epoch: Here the model was trained for only 1 epoch due to computation constraints. However, there is potential for further training, if the validation loss continues to decrease while the training loss stabilizes. \n",
    "\n",
    "Monitoring: For monitoring the training\n",
    "process we could use of Weights and Biases, however skipping that due to environment contraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86faca01",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc5d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model\n",
    "model_path = \"model\"\n",
    "model_filename = f\"epoch_0_trainloss_0-05_valloss_0-04\"\n",
    "model_filepath = os.path.join(model_path, model_filename)\n",
    "model = TextClassifier(model_directory=model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82815887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Convert index to labels for vizualisation\n",
    "y_true = [idx_to_label[i] for i in y_val.to_list()]\n",
    "y_pred = [idx_to_label[i] for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a52d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiclass_classification(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate a multi-class classification model.\n",
    "\n",
    "    Args:\n",
    "    - y_true (List[Union[int, str]]): True labels.\n",
    "    - y_pred (List[Union[int, str]]): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    - metrics (Dict[str, Union[float, str, List[List[int]], str]]): A dictionary containing the evaluation metrics.\n",
    "      - \"Accuracy\" (float): Accuracy score.\n",
    "      - \"Precision (weighted)\" (float): Weighted precision score.\n",
    "      - \"Recall (weighted)\" (float): Weighted recall score.\n",
    "      - \"F1 Score (weighted)\" (float): Weighted F1 score.\n",
    "      - \"Confusion Matrix\" (List[List[int]]): Confusion matrix.\n",
    "      - \"Classification Report\" (str): Classification report as a string.\n",
    "    \"\"\"\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # Calculate precision, recall, and F1-score for each class\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    # Calculate the confusion matrix\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    # Generate a classification report\n",
    "    class_report = classification_report(y_true, y_pred, target_names=sorted(set(y_true)))\n",
    "    # Create a dictionary to store the results\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision (weighted)\": precision,\n",
    "        \"Recall (weighted)\": recall,\n",
    "        \"F1 Score (weighted)\": f1,\n",
    "        \"Confusion Matrix\": confusion,\n",
    "        \"Classification Report\": class_report\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "result = evaluate_multiclass_classification(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25736081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print evaluations results\n",
    "for key, value in result.items():\n",
    "    print(f'{key}: \\t {value} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a274a88b",
   "metadata": {},
   "source": [
    "***Notes:***\n",
    "\n",
    "The model is giving good performance of 0.95 + F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8b4af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix to visualize the performance of a classification model.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): True class labels.\n",
    "    y_pred (array-like): Predicted class labels.\n",
    "    classes (array-like): List of class labels.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.set(font_scale=1.2)  # Adjust font size\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "class_names = sorted(set(y_true))\n",
    "plot_confusion_matrix(y_true, y_pred, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eac54b",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c125af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data\n",
    "test = pd.read_csv(os.path.join(\"test\",\"test.csv\"))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e292e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating title and content\n",
    "test['text'] = test.apply(reformat_dataframe, axis=1)\n",
    "# Pre-processing text\n",
    "test['text'] = test['text'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c309714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model\n",
    "model_path = \"model\"\n",
    "model_filename = f\"epoch_0_trainloss_0-05_valloss_0-04\"\n",
    "model_filepath = os.path.join(model_path, model_filename)\n",
    "model = TextClassifier(model_directory=model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22f9085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "test['category'] = model.predict(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subset column for submission file\n",
    "submissions_df = test[['id', 'category']].copy()\n",
    "submissions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80647a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save submission result\n",
    "submissions_df.to_csv('submissions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb3e072",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1622e",
   "metadata": {},
   "source": [
    "***Notes:***\n",
    "\n",
    "Some of the further Improvement areas include:\n",
    "- Modularize the code further in classes.\n",
    "- Use WandB for training and logging experiment.\n",
    "- Write unit test for the functions to make code robust.\n",
    "- Train the model for longer.\n",
    "- Try other models like RoBERTa, domain specific models mentioned in the notebook.\n",
    "- Do error analysis on misclassification for further improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
